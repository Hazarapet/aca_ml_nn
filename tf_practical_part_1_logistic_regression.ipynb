{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow practical, part 1, logistic regression\n",
    "\n",
    "This code sample gives an example implementation for logistic regression (see the comments in the code for details) for MNIST dataset.\n",
    "Your task is to play with the code and understand it so that you could implement and debug the next part yourself. Do the following small tasks step by step:\n",
    "* Compute and print/plot logits for a batch of examples. You need to add it to the list of the computed values for that.\n",
    "* Play with weights initializer. Learn how to initialize it with constant or random values of different standard deviation.\n",
    "* Print the weights \"W\" while traning (Hint: Tensor.eval() operation is useful)\n",
    "* Play with different optimizers. Now you can use momentum, Adagrad or ADAM, or any other method without a need to implement it.\n",
    "* Try using special functions for loss value. Check out the functions ```softmax_cross_entropy_with_logits```, ```sigmoid_cross_entropy_with_logits```.\n",
    "* Produce one-hot vectors for the labels using ```tf.one_hot```.\n",
    "* Implement multi-class case logistic regression.\n",
    "* Print or plot the gradient of the loss function using ```tf.grad```.\n",
    "* Print any intermediate gradient e.g. $\\frac{\\partial p}{\\partial W}$. Does the dimensionality match your expectations? Now you can do it easily for any pair of variables in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_batch = 4;\n",
    "n_classes = 2;\n",
    "n_feat = 784;\n",
    "\n",
    "# set random seed to reproduce the results if necessary\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# this is needed to avoid clashes of names if there was a previous version of the same model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# two placeholders for the input and the target\n",
    "X = tf.placeholder(tf.float32, shape=[n_batch, n_feat])\n",
    "target = tf.placeholder(tf.float32, shape=[n_batch, 1])\n",
    "\n",
    "# declare the variable for weights and biases\n",
    "W = tf.Variable(tf.random_normal([n_feat, 1]),name=\"W\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"b\")\n",
    "\n",
    "# matrix vector multiplication\n",
    "logits = tf.matmul(X,W) + b\n",
    "\n",
    "# this would be useful for multi-class classification\n",
    "# target_one_hot = tf.one_hot(target, n_classes)\n",
    "\n",
    "# non-linearities are declared in tf.nn \n",
    "p = tf.nn.sigmoid(logits);\n",
    "\n",
    "# TensorFlow is not flexible for types. Explicit conversion is necessary each time\n",
    "correct_pred = tf.equal(tf.cast(tf.round(p), \"int64\"), tf.cast(target, \"int64\"))\n",
    "\n",
    "# reduction operations like mean and sum are done in numpy style\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "\n",
    "loss = -1.0 * tf.reduce_sum(target * tf.log(p + 1.0E-8) + (1.0 - target) * tf.log(1.0 - p + 1.0E-8))\n",
    "\n",
    "# this line defines the operation for a single optimization step\n",
    "# you only need to choose the method and specify the set of parameters\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-73ef14b0c949>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# this code reads the data from tensorflow examples datasets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def load_mnist_dataset(binary, label1, label2):\n",
    "    mnist = input_data.read_data_sets(\"Data/MNIST_data/\", reshape=True, one_hot=False)\n",
    " \n",
    "    X_train = mnist.train.images\n",
    "    target_train = mnist.train.labels\n",
    "    X_test = mnist.test.images\n",
    "    target_test = mnist.test.labels\n",
    "\n",
    "    if (binary):\n",
    "        # Get only the samples with zero and one label for training.\n",
    "        index_list_train = []\n",
    "        for sample_index in range(target_train.shape[0]):\n",
    "            label = target_train[sample_index]\n",
    "            #if label == 1 or label == 0:\n",
    "            if (label == label1 or label == label2):\n",
    "                index_list_train.append(sample_index)\n",
    "\n",
    "        # Reform the train data structure.\n",
    "        X_train = mnist.train.images[index_list_train]\n",
    "        target_train = mnist.train.labels[index_list_train]\n",
    "\n",
    "        # Get only the samples with four and nine label for test set.\n",
    "        index_list_test = []\n",
    "        for sample_index in range(target_test.shape[0]):\n",
    "            label = target_test[sample_index]\n",
    "            #if label == 1 or label == 0:\n",
    "            if (label == label1 or label == label2):\n",
    "                index_list_test.append(sample_index)    \n",
    "\n",
    "        # Reform the test data structure.\n",
    "        X_test = mnist.test.images[index_list_test]\n",
    "        target_test = mnist.test.labels[index_list_test]\n",
    "\n",
    "        target_train[target_train == label1] = 0;\n",
    "        target_train[target_train == label2] = 1;\n",
    "        \n",
    "        target_test[target_test == label1] = 0;\n",
    "        target_test[target_test == label2] = 1;\n",
    "        \n",
    "    return (X_train, X_test, target_train, target_test)\n",
    "\n",
    "(X_train, X_test, target_train, target_test) = load_mnist_dataset(binary = True, label1 = 4, label2 = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 1.49999, train acc: 0.887268\n",
      "epoch 1, train loss: 0.686929, train acc: 0.944888\n",
      "epoch 2, train loss: 0.589441, train acc: 0.952974\n",
      "epoch 3, train loss: 0.537098, train acc: 0.957249\n",
      "epoch 4, train loss: 0.501773, train acc: 0.961338\n",
      "epoch 5, train loss: 0.475544, train acc: 0.962825\n",
      "epoch 6, train loss: 0.454903, train acc: 0.96487\n",
      "epoch 7, train loss: 0.438087, train acc: 0.965985\n",
      "epoch 8, train loss: 0.424018, train acc: 0.966822\n",
      "epoch 9, train loss: 0.412001, train acc: 0.967844\n",
      "epoch 10, train loss: 0.401599, train acc: 0.96868\n",
      "epoch 11, train loss: 0.392468, train acc: 0.968959\n",
      "epoch 12, train loss: 0.384368, train acc: 0.969238\n",
      "epoch 13, train loss: 0.377122, train acc: 0.969888\n",
      "epoch 14, train loss: 0.370593, train acc: 0.970725\n",
      "epoch 15, train loss: 0.364673, train acc: 0.97119\n",
      "epoch 16, train loss: 0.359276, train acc: 0.972026\n",
      "epoch 17, train loss: 0.35433, train acc: 0.972026\n",
      "epoch 18, train loss: 0.349783, train acc: 0.972584\n",
      "epoch 19, train loss: 0.345582, train acc: 0.972677\n"
     ]
    }
   ],
   "source": [
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "target_train = target_train.reshape((n_train,1))\n",
    "target_test = target_test.reshape((n_test,1))\n",
    "\n",
    "n_epoch = 20\n",
    "\n",
    "# define a TF session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())   \n",
    "    \n",
    "    indices = np.arange(n_train)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # use you implementation of iterate_minibatches function here\n",
    "\n",
    "    loss_train_epoch = {}\n",
    "    \n",
    "    for i_epoch in range(0,n_epoch):\n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "\n",
    "        rng_minibatches = range(0, n_train - n_batch + 1, n_batch);\n",
    "        \n",
    "        for start_idx in rng_minibatches:\n",
    "            X_train_batch = X_train[start_idx:(start_idx+n_batch),:]\n",
    "            target_batch = target_train[start_idx:(start_idx+n_batch),:]    \n",
    "            \n",
    "            # run the session\n",
    "            # the list of variable [loss, acc, p] is the list of tensors you would like to compute\n",
    "            # the list of values [loss_val,acc_val,p_val] that are computed is defined on the left\n",
    "            # feed_dict = {} is the way to provide input data for the placeholders you have in the model\n",
    "            [loss_val,acc_val,p_val] = sess.run([loss,acc,p], feed_dict={X:X_train_batch, target:target_batch})\n",
    "            \n",
    "            # run a single step of the optimizer you specified in the model given the input\n",
    "            train_step.run(feed_dict={X: X_train_batch, target: target_batch})\n",
    "            \n",
    "            acc_train += acc_val\n",
    "            loss_train += loss_val\n",
    "            \n",
    "        loss_train_epoch[i_epoch] = loss_train / len(rng_minibatches)\n",
    "        \n",
    "        # implement computing the loss function and accuracy for the test set\n",
    "\n",
    "        print(\"epoch %d, train loss: %g, train acc: %g\" % (i_epoch, loss_train / len(rng_minibatches), acc_train / len(rng_minibatches)))\n",
    "\n",
    "        # use your favourite tool to make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
